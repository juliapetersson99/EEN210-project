{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./Train_data/Jacob_running.csv\n",
      "The average frequency is approximately 59.08 Hz\n",
      "Loaded 5820 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 193 windows\n",
      "Loading ./Train_data/Jacob_first_gym.csv\n",
      "The average frequency is approximately 59.65 Hz\n",
      "Loaded 7105 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 235 windows\n",
      "Loading ./Train_data/Julia_first_gym.csv\n",
      "The average frequency is approximately 61.00 Hz\n",
      "Loaded 5993 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 198 windows\n",
      "Loading ./Train_data/Julia_running.csv\n",
      "The average frequency is approximately 54.71 Hz\n",
      "Loaded 4971 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 164 windows\n",
      "Loading ./Train_data/julia_sitting_to_fall.csv\n",
      "The average frequency is approximately 60.64 Hz\n",
      "Loaded 4987 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 165 windows\n",
      "Loading ./Train_data/Marten_second.csv\n",
      "The average frequency is approximately 60.08 Hz\n",
      "Loaded 4800 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 159 windows\n",
      "Loading ./Train_data/Marten_first.csv\n",
      "The average frequency is approximately 60.96 Hz\n",
      "Loaded 2000 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 65 windows\n",
      "Loading ./Train_data/sara_first.csv\n",
      "The average frequency is approximately 60.48 Hz\n",
      "Loaded 4659 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 154 windows\n",
      "Loading ./Train_data/Marten_running.csv\n",
      "The average frequency is approximately 58.35 Hz\n",
      "Loaded 5537 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 183 windows\n",
      "Loading ./Train_data/Marten_first_gym.csv\n",
      "The average frequency is approximately 60.73 Hz\n",
      "Loaded 5625 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 186 windows\n",
      "Loading ./Train_data/Sara_first_gym.csv\n",
      "The average frequency is approximately 60.29 Hz\n",
      "Loaded 12038 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 400 windows\n",
      "Loading ./Train_data/Jacob_second_gym.csv\n",
      "The average frequency is approximately 53.60 Hz\n",
      "Loaded 3724 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 123 windows\n",
      "Loading ./Train_data/Julia_first.csv\n",
      "The average frequency is approximately 59.65 Hz\n",
      "Loaded 5800 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 192 windows\n",
      "Loading ./Train_data/Jacob_first.csv\n",
      "The average frequency is approximately 60.73 Hz\n",
      "Loaded 4700 samples\n",
      "Applied low-pass filter\n",
      "Segmented into 155 windows\n",
      "After normalization:\n",
      "acceleration_x: min = 0.0000, max = 1.0000\n",
      "acceleration_y: min = 0.0000, max = 1.0000\n",
      "acceleration_z: min = 0.0000, max = 1.0000\n",
      "gyroscope_x: min = 0.0000, max = 1.0000\n",
      "gyroscope_y: min = 0.0000, max = 1.0000\n",
      "gyroscope_z: min = 0.0000, max = 1.0000\n",
      "Label to index mapping: {np.str_('falling'): 0, np.str_('lying'): 1, np.str_('recover'): 2, np.str_('running'): 3, np.str_('sitting'): 4, np.str_('standing'): 5, np.str_('walking'): 6}\n",
      "X shape: (2572, 60, 6)\n",
      "y shape: (2572,)\n",
      "Number of data points per class: {np.int64(0): np.int64(143), np.int64(1): np.int64(185), np.int64(2): np.int64(210), np.int64(3): np.int64(100), np.int64(4): np.int64(375), np.int64(5): np.int64(896), np.int64(6): np.int64(663)}\n"
     ]
    }
   ],
   "source": [
    "import process as pp\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import CNN_LSTM\n",
    "from CNN_LSTM import CNN_LSTM  # Add this line to import the class specifically\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from CNN_LSTM_2 import CNN_LSTM_2 # \n",
    "from collections import Counter\n",
    "\n",
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, windows, labels):\n",
    "        self.windows = windows\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = torch.tensor(self.windows[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return window, label\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_channels = 6\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "lr = 0.0001\n",
    "\n",
    "fs = 60\n",
    "window_duration_sec = 1\n",
    "overlap = 0.5\n",
    "\n",
    "# Load and pre-process the data.\n",
    "X, y = pp.load_and_preprocess_data('./Train_data', window_duration_sec, fs, overlap)\n",
    "y_encoded, label_to_idx = pp.encode_labels(y)\n",
    "print(\"Label to index mapping:\", label_to_idx)\n",
    "print(\"X shape:\", X.shape)  \n",
    "print(\"y shape:\", y_encoded.shape)\n",
    "\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Number of data points per class:\", class_counts)\n",
    "\n",
    "\n",
    "n_samples, window_length, num_channels = X.shape\n",
    "#X_flat = X.reshape(n_samples, window_length * num_channels)\n",
    "#print(\"Flattened X shape:\", X_flat.shape)  \n",
    "#Smote, not sure if it works now when we have timne series data\n",
    "#smote = SMOTE(random_state=198)\n",
    "\n",
    "#X_res_flat, y_res = smote.fit_resample(X_flat, y)\n",
    "#print(\"New label distribution:\", Counter(y_res))\n",
    "#X_res = X_res_flat.reshape(-1, window_length, num_channels)\n",
    "#print(\"Resampled X shape:\", X_res.shape)\n",
    "\n",
    "# Split into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=19, stratify=y_encoded)\n",
    "\n",
    "window_length = fs * window_duration_sec\n",
    "\n",
    "#ok to shuffel as each window should be fine to be shuffled\n",
    "train_dataset = SensorDataset(X_train, y_train)\n",
    "test_dataset = SensorDataset(X_test, y_test)\n",
    "\n",
    "# Wrap the datasets in DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/1000] Train Loss: 1.8982, Train Acc: 28.49% | Test Loss: 1.7285, Test Acc: 34.76%\n",
      "Epoch [2/1000] Train Loss: 1.6902, Train Acc: 34.86% | Test Loss: 1.6810, Test Acc: 34.76%\n",
      "Epoch [3/1000] Train Loss: 1.6807, Train Acc: 34.86% | Test Loss: 1.6798, Test Acc: 34.76%\n",
      "Epoch [4/1000] Train Loss: 1.6799, Train Acc: 34.86% | Test Loss: 1.6796, Test Acc: 34.76%\n",
      "Epoch [5/1000] Train Loss: 1.6806, Train Acc: 34.86% | Test Loss: 1.6794, Test Acc: 34.76%\n",
      "Epoch [6/1000] Train Loss: 1.6804, Train Acc: 34.86% | Test Loss: 1.6794, Test Acc: 34.76%\n",
      "Epoch [7/1000] Train Loss: 1.6800, Train Acc: 34.86% | Test Loss: 1.6794, Test Acc: 34.76%\n",
      "Epoch [8/1000] Train Loss: 1.6784, Train Acc: 34.86% | Test Loss: 1.6717, Test Acc: 34.76%\n",
      "Epoch [9/1000] Train Loss: 1.5940, Train Acc: 34.86% | Test Loss: 1.4602, Test Acc: 34.76%\n",
      "Epoch [10/1000] Train Loss: 1.6120, Train Acc: 34.86% | Test Loss: 1.5612, Test Acc: 34.76%\n",
      "Epoch [11/1000] Train Loss: 1.4594, Train Acc: 34.86% | Test Loss: 1.5104, Test Acc: 34.76%\n",
      "Epoch [12/1000] Train Loss: 1.4665, Train Acc: 35.05% | Test Loss: 1.4140, Test Acc: 36.12%\n",
      "Epoch [13/1000] Train Loss: 1.4439, Train Acc: 36.56% | Test Loss: 1.4425, Test Acc: 36.89%\n",
      "Epoch [14/1000] Train Loss: 1.4266, Train Acc: 37.19% | Test Loss: 1.4436, Test Acc: 38.83%\n",
      "Epoch [15/1000] Train Loss: 1.4092, Train Acc: 39.43% | Test Loss: 1.3326, Test Acc: 41.55%\n",
      "Epoch [16/1000] Train Loss: 1.2943, Train Acc: 46.86% | Test Loss: 1.2478, Test Acc: 51.26%\n",
      "Epoch [17/1000] Train Loss: 1.2815, Train Acc: 51.34% | Test Loss: 1.5965, Test Acc: 31.07%\n",
      "Epoch [18/1000] Train Loss: 1.2967, Train Acc: 47.93% | Test Loss: 1.1929, Test Acc: 52.82%\n",
      "Epoch [19/1000] Train Loss: 1.2458, Train Acc: 51.87% | Test Loss: 1.1454, Test Acc: 59.03%\n",
      "Epoch [20/1000] Train Loss: 1.2111, Train Acc: 52.07% | Test Loss: 1.1299, Test Acc: 59.22%\n",
      "Epoch [21/1000] Train Loss: 1.1876, Train Acc: 53.43% | Test Loss: 1.1651, Test Acc: 53.98%\n",
      "Epoch [22/1000] Train Loss: 1.1360, Train Acc: 56.73% | Test Loss: 1.1006, Test Acc: 60.19%\n",
      "Epoch [23/1000] Train Loss: 1.1452, Train Acc: 55.37% | Test Loss: 1.2295, Test Acc: 49.51%\n",
      "Epoch [24/1000] Train Loss: 1.1180, Train Acc: 56.73% | Test Loss: 1.0896, Test Acc: 60.19%\n",
      "Epoch [25/1000] Train Loss: 1.1552, Train Acc: 54.79% | Test Loss: 1.0917, Test Acc: 60.19%\n",
      "Epoch [26/1000] Train Loss: 1.1039, Train Acc: 57.85% | Test Loss: 1.3290, Test Acc: 48.16%\n",
      "Epoch [27/1000] Train Loss: 1.1236, Train Acc: 56.30% | Test Loss: 1.0956, Test Acc: 60.39%\n",
      "Epoch [28/1000] Train Loss: 1.1188, Train Acc: 55.91% | Test Loss: 1.0949, Test Acc: 57.67%\n",
      "Epoch [29/1000] Train Loss: 1.1000, Train Acc: 57.27% | Test Loss: 1.0969, Test Acc: 58.06%\n",
      "Epoch [30/1000] Train Loss: 1.0772, Train Acc: 58.77% | Test Loss: 1.0509, Test Acc: 60.78%\n",
      "Epoch [31/1000] Train Loss: 1.0991, Train Acc: 58.00% | Test Loss: 1.0503, Test Acc: 60.78%\n",
      "Epoch [32/1000] Train Loss: 1.0956, Train Acc: 57.32% | Test Loss: 1.1103, Test Acc: 56.89%\n",
      "Epoch [33/1000] Train Loss: 1.0966, Train Acc: 58.58% | Test Loss: 1.1080, Test Acc: 55.53%\n",
      "Epoch [34/1000] Train Loss: 1.0539, Train Acc: 59.46% | Test Loss: 1.0250, Test Acc: 61.75%\n",
      "Epoch [35/1000] Train Loss: 1.0424, Train Acc: 60.43% | Test Loss: 1.0398, Test Acc: 61.17%\n",
      "Epoch [36/1000] Train Loss: 1.0552, Train Acc: 59.70% | Test Loss: 1.0401, Test Acc: 60.58%\n",
      "Epoch [37/1000] Train Loss: 1.0843, Train Acc: 58.34% | Test Loss: 1.0488, Test Acc: 60.58%\n",
      "Epoch [38/1000] Train Loss: 1.0427, Train Acc: 59.75% | Test Loss: 1.0202, Test Acc: 63.11%\n",
      "Epoch [39/1000] Train Loss: 1.0512, Train Acc: 59.65% | Test Loss: 1.1798, Test Acc: 52.04%\n",
      "Epoch [40/1000] Train Loss: 1.0343, Train Acc: 60.04% | Test Loss: 1.0308, Test Acc: 61.17%\n",
      "Epoch [41/1000] Train Loss: 1.0546, Train Acc: 59.60% | Test Loss: 1.0274, Test Acc: 61.17%\n",
      "Epoch [42/1000] Train Loss: 1.0271, Train Acc: 60.67% | Test Loss: 1.0182, Test Acc: 61.55%\n",
      "Epoch [43/1000] Train Loss: 1.0208, Train Acc: 62.13% | Test Loss: 1.0752, Test Acc: 59.81%\n",
      "Epoch [44/1000] Train Loss: 1.0399, Train Acc: 59.94% | Test Loss: 0.9998, Test Acc: 63.30%\n",
      "Epoch [45/1000] Train Loss: 1.0112, Train Acc: 62.52% | Test Loss: 1.0104, Test Acc: 62.52%\n",
      "Epoch [46/1000] Train Loss: 1.0055, Train Acc: 61.79% | Test Loss: 0.9990, Test Acc: 62.91%\n",
      "Epoch [47/1000] Train Loss: 1.0378, Train Acc: 60.67% | Test Loss: 0.9971, Test Acc: 63.50%\n",
      "Epoch [48/1000] Train Loss: 1.0277, Train Acc: 60.04% | Test Loss: 1.1434, Test Acc: 53.20%\n",
      "Epoch [49/1000] Train Loss: 1.0226, Train Acc: 60.72% | Test Loss: 1.1178, Test Acc: 56.31%\n",
      "Epoch [50/1000] Train Loss: 1.0254, Train Acc: 61.21% | Test Loss: 1.0735, Test Acc: 56.50%\n",
      "Epoch [51/1000] Train Loss: 1.0052, Train Acc: 61.21% | Test Loss: 1.0001, Test Acc: 62.33%\n",
      "Epoch [52/1000] Train Loss: 0.9860, Train Acc: 61.59% | Test Loss: 1.0800, Test Acc: 58.25%\n",
      "Epoch [53/1000] Train Loss: 0.9925, Train Acc: 62.81% | Test Loss: 1.0946, Test Acc: 56.31%\n",
      "Epoch [54/1000] Train Loss: 0.9799, Train Acc: 62.71% | Test Loss: 0.9879, Test Acc: 63.30%\n",
      "Epoch [55/1000] Train Loss: 1.0195, Train Acc: 61.11% | Test Loss: 1.1074, Test Acc: 57.48%\n",
      "Epoch [56/1000] Train Loss: 1.0297, Train Acc: 60.67% | Test Loss: 0.9738, Test Acc: 65.44%\n",
      "Epoch [57/1000] Train Loss: 0.9921, Train Acc: 62.71% | Test Loss: 0.9734, Test Acc: 64.66%\n",
      "Epoch [58/1000] Train Loss: 0.9800, Train Acc: 62.86% | Test Loss: 0.9654, Test Acc: 64.66%\n",
      "Epoch [59/1000] Train Loss: 0.9920, Train Acc: 62.32% | Test Loss: 0.9682, Test Acc: 65.44%\n",
      "Epoch [60/1000] Train Loss: 0.9760, Train Acc: 63.15% | Test Loss: 1.1169, Test Acc: 57.86%\n",
      "Epoch [61/1000] Train Loss: 1.0155, Train Acc: 61.40% | Test Loss: 0.9851, Test Acc: 62.52%\n",
      "Epoch [62/1000] Train Loss: 0.9709, Train Acc: 63.25% | Test Loss: 0.9652, Test Acc: 65.24%\n",
      "Epoch [63/1000] Train Loss: 0.9796, Train Acc: 62.96% | Test Loss: 0.9736, Test Acc: 63.88%\n",
      "Epoch [64/1000] Train Loss: 0.9529, Train Acc: 63.98% | Test Loss: 0.9451, Test Acc: 65.63%\n",
      "Epoch [65/1000] Train Loss: 0.9457, Train Acc: 64.90% | Test Loss: 0.9651, Test Acc: 65.24%\n",
      "Epoch [66/1000] Train Loss: 1.0146, Train Acc: 60.82% | Test Loss: 1.0072, Test Acc: 61.75%\n",
      "Epoch [67/1000] Train Loss: 0.9604, Train Acc: 63.54% | Test Loss: 0.9553, Test Acc: 63.88%\n",
      "Epoch [68/1000] Train Loss: 0.9451, Train Acc: 63.98% | Test Loss: 1.1282, Test Acc: 56.50%\n",
      "Epoch [69/1000] Train Loss: 0.9660, Train Acc: 63.39% | Test Loss: 0.9726, Test Acc: 62.52%\n",
      "Epoch [70/1000] Train Loss: 0.9455, Train Acc: 63.78% | Test Loss: 0.9444, Test Acc: 65.83%\n",
      "Epoch [71/1000] Train Loss: 0.9459, Train Acc: 64.12% | Test Loss: 0.9265, Test Acc: 65.63%\n",
      "Epoch [72/1000] Train Loss: 0.9981, Train Acc: 61.79% | Test Loss: 0.9424, Test Acc: 66.21%\n",
      "Epoch [73/1000] Train Loss: 0.9604, Train Acc: 63.30% | Test Loss: 0.9381, Test Acc: 64.66%\n",
      "Epoch [74/1000] Train Loss: 0.9396, Train Acc: 64.66% | Test Loss: 0.9281, Test Acc: 66.02%\n",
      "Epoch [75/1000] Train Loss: 0.9324, Train Acc: 65.00% | Test Loss: 0.9763, Test Acc: 63.69%\n",
      "Epoch [76/1000] Train Loss: 0.9836, Train Acc: 62.08% | Test Loss: 0.9204, Test Acc: 66.02%\n",
      "Epoch [77/1000] Train Loss: 0.9389, Train Acc: 64.37% | Test Loss: 1.0688, Test Acc: 56.89%\n",
      "Epoch [78/1000] Train Loss: 0.9457, Train Acc: 64.41% | Test Loss: 0.9366, Test Acc: 64.27%\n",
      "Epoch [79/1000] Train Loss: 0.9311, Train Acc: 64.75% | Test Loss: 0.9237, Test Acc: 66.80%\n",
      "Epoch [80/1000] Train Loss: 0.9315, Train Acc: 65.19% | Test Loss: 0.9072, Test Acc: 66.60%\n",
      "Epoch [81/1000] Train Loss: 0.9317, Train Acc: 65.24% | Test Loss: 0.9544, Test Acc: 64.85%\n",
      "Epoch [82/1000] Train Loss: 0.9076, Train Acc: 65.97% | Test Loss: 1.0956, Test Acc: 58.06%\n",
      "Epoch [83/1000] Train Loss: 0.9717, Train Acc: 63.49% | Test Loss: 0.9222, Test Acc: 67.38%\n",
      "Epoch [84/1000] Train Loss: 0.9246, Train Acc: 64.71% | Test Loss: 0.9169, Test Acc: 66.41%\n",
      "Epoch [85/1000] Train Loss: 0.9046, Train Acc: 66.31% | Test Loss: 0.9972, Test Acc: 62.72%\n",
      "Epoch [86/1000] Train Loss: 0.9227, Train Acc: 64.95% | Test Loss: 0.9316, Test Acc: 65.83%\n",
      "Epoch [87/1000] Train Loss: 0.9082, Train Acc: 65.73% | Test Loss: 0.9690, Test Acc: 63.11%\n",
      "Epoch [88/1000] Train Loss: 0.9017, Train Acc: 65.73% | Test Loss: 0.9089, Test Acc: 65.63%\n",
      "Epoch [89/1000] Train Loss: 0.9011, Train Acc: 65.87% | Test Loss: 0.8810, Test Acc: 65.83%\n",
      "Epoch [90/1000] Train Loss: 0.8905, Train Acc: 66.99% | Test Loss: 0.9239, Test Acc: 65.83%\n",
      "Epoch [91/1000] Train Loss: 0.9609, Train Acc: 64.37% | Test Loss: 1.0247, Test Acc: 61.17%\n",
      "Epoch [92/1000] Train Loss: 0.9098, Train Acc: 66.26% | Test Loss: 0.8971, Test Acc: 66.60%\n",
      "Epoch [93/1000] Train Loss: 0.8932, Train Acc: 66.75% | Test Loss: 0.8881, Test Acc: 66.41%\n",
      "Epoch [94/1000] Train Loss: 0.9013, Train Acc: 66.41% | Test Loss: 0.9304, Test Acc: 66.02%\n",
      "Epoch [95/1000] Train Loss: 0.9076, Train Acc: 66.36% | Test Loss: 0.9029, Test Acc: 66.60%\n",
      "Epoch [96/1000] Train Loss: 0.9557, Train Acc: 64.07% | Test Loss: 1.0235, Test Acc: 61.36%\n",
      "Epoch [97/1000] Train Loss: 0.9335, Train Acc: 64.22% | Test Loss: 0.9404, Test Acc: 65.44%\n",
      "Epoch [98/1000] Train Loss: 0.8954, Train Acc: 67.23% | Test Loss: 0.8990, Test Acc: 68.35%\n",
      "Epoch [99/1000] Train Loss: 0.8998, Train Acc: 66.02% | Test Loss: 0.8956, Test Acc: 67.18%\n",
      "Epoch [100/1000] Train Loss: 0.8787, Train Acc: 67.28% | Test Loss: 0.9292, Test Acc: 66.41%\n",
      "Epoch [101/1000] Train Loss: 0.8671, Train Acc: 67.72% | Test Loss: 0.9093, Test Acc: 65.83%\n",
      "Epoch [102/1000] Train Loss: 0.8513, Train Acc: 68.45% | Test Loss: 0.9080, Test Acc: 66.41%\n",
      "Epoch [103/1000] Train Loss: 0.8964, Train Acc: 67.33% | Test Loss: 0.8489, Test Acc: 67.96%\n",
      "Epoch [104/1000] Train Loss: 0.8517, Train Acc: 68.30% | Test Loss: 0.8643, Test Acc: 66.21%\n",
      "Epoch [105/1000] Train Loss: 0.8628, Train Acc: 67.67% | Test Loss: 0.8794, Test Acc: 67.18%\n",
      "Epoch [106/1000] Train Loss: 0.8446, Train Acc: 68.30% | Test Loss: 0.8463, Test Acc: 68.74%\n",
      "Epoch [107/1000] Train Loss: 0.8726, Train Acc: 67.62% | Test Loss: 0.8692, Test Acc: 67.38%\n",
      "Epoch [108/1000] Train Loss: 0.8389, Train Acc: 68.30% | Test Loss: 0.8381, Test Acc: 67.57%\n",
      "Epoch [109/1000] Train Loss: 0.8598, Train Acc: 68.21% | Test Loss: 0.8653, Test Acc: 66.60%\n",
      "Epoch [110/1000] Train Loss: 0.8657, Train Acc: 67.04% | Test Loss: 0.8667, Test Acc: 67.57%\n",
      "Epoch [111/1000] Train Loss: 0.8369, Train Acc: 68.45% | Test Loss: 0.8781, Test Acc: 66.60%\n",
      "Epoch [112/1000] Train Loss: 0.8707, Train Acc: 67.48% | Test Loss: 0.9308, Test Acc: 62.72%\n",
      "Epoch [113/1000] Train Loss: 0.8395, Train Acc: 69.08% | Test Loss: 0.8594, Test Acc: 67.18%\n",
      "Epoch [114/1000] Train Loss: 0.8509, Train Acc: 67.91% | Test Loss: 0.8491, Test Acc: 68.35%\n",
      "Epoch [115/1000] Train Loss: 0.8269, Train Acc: 69.08% | Test Loss: 0.8521, Test Acc: 67.57%\n",
      "Epoch [116/1000] Train Loss: 0.8910, Train Acc: 66.60% | Test Loss: 0.8412, Test Acc: 68.16%\n",
      "Epoch [117/1000] Train Loss: 0.8317, Train Acc: 69.18% | Test Loss: 0.8362, Test Acc: 67.57%\n",
      "Epoch [118/1000] Train Loss: 0.8624, Train Acc: 67.72% | Test Loss: 0.8253, Test Acc: 68.74%\n",
      "Epoch [119/1000] Train Loss: 0.8159, Train Acc: 68.30% | Test Loss: 0.8155, Test Acc: 68.16%\n",
      "Epoch [120/1000] Train Loss: 0.7985, Train Acc: 69.47% | Test Loss: 0.7851, Test Acc: 70.68%\n",
      "Epoch [121/1000] Train Loss: 0.7928, Train Acc: 70.54% | Test Loss: 0.8757, Test Acc: 66.99%\n",
      "Epoch [122/1000] Train Loss: 0.8293, Train Acc: 68.74% | Test Loss: 0.8944, Test Acc: 68.93%\n",
      "Epoch [123/1000] Train Loss: 0.8439, Train Acc: 68.74% | Test Loss: 0.7508, Test Acc: 72.23%\n",
      "Epoch [124/1000] Train Loss: 0.7620, Train Acc: 72.68% | Test Loss: 0.8067, Test Acc: 67.77%\n",
      "Epoch [125/1000] Train Loss: 0.8037, Train Acc: 70.54% | Test Loss: 0.8314, Test Acc: 69.32%\n",
      "Epoch [126/1000] Train Loss: 0.7809, Train Acc: 71.46% | Test Loss: 0.7504, Test Acc: 74.37%\n",
      "Epoch [127/1000] Train Loss: 0.7450, Train Acc: 73.51% | Test Loss: 0.7489, Test Acc: 73.98%\n",
      "Epoch [128/1000] Train Loss: 0.7558, Train Acc: 72.97% | Test Loss: 0.7502, Test Acc: 71.26%\n",
      "Epoch [129/1000] Train Loss: 0.7411, Train Acc: 72.58% | Test Loss: 0.7546, Test Acc: 73.98%\n",
      "Epoch [130/1000] Train Loss: 0.7193, Train Acc: 74.57% | Test Loss: 0.7499, Test Acc: 74.17%\n",
      "Epoch [131/1000] Train Loss: 0.7130, Train Acc: 73.89% | Test Loss: 0.7435, Test Acc: 72.23%\n",
      "Epoch [132/1000] Train Loss: 0.7684, Train Acc: 71.75% | Test Loss: 0.7418, Test Acc: 71.26%\n",
      "Epoch [133/1000] Train Loss: 0.7322, Train Acc: 73.70% | Test Loss: 0.8236, Test Acc: 68.16%\n",
      "Epoch [134/1000] Train Loss: 0.7723, Train Acc: 72.24% | Test Loss: 0.8140, Test Acc: 69.51%\n",
      "Epoch [135/1000] Train Loss: 0.7604, Train Acc: 72.29% | Test Loss: 0.8412, Test Acc: 67.57%\n",
      "Epoch [136/1000] Train Loss: 0.7095, Train Acc: 74.23% | Test Loss: 0.6858, Test Acc: 74.37%\n",
      "Epoch [137/1000] Train Loss: 0.7200, Train Acc: 73.31% | Test Loss: 0.8040, Test Acc: 69.71%\n",
      "Epoch [138/1000] Train Loss: 0.7302, Train Acc: 72.97% | Test Loss: 0.6985, Test Acc: 73.59%\n",
      "Epoch [139/1000] Train Loss: 0.7061, Train Acc: 73.55% | Test Loss: 0.7194, Test Acc: 72.04%\n",
      "Epoch [140/1000] Train Loss: 0.7065, Train Acc: 72.87% | Test Loss: 0.7019, Test Acc: 72.82%\n",
      "Epoch [141/1000] Train Loss: 0.7165, Train Acc: 74.19% | Test Loss: 0.7095, Test Acc: 74.95%\n",
      "Epoch [142/1000] Train Loss: 0.6860, Train Acc: 74.57% | Test Loss: 0.7973, Test Acc: 72.23%\n",
      "Epoch [143/1000] Train Loss: 0.7005, Train Acc: 73.65% | Test Loss: 0.6990, Test Acc: 74.56%\n",
      "Epoch [144/1000] Train Loss: 0.7037, Train Acc: 73.89% | Test Loss: 0.6515, Test Acc: 74.37%\n",
      "Epoch [145/1000] Train Loss: 0.7116, Train Acc: 74.33% | Test Loss: 0.7137, Test Acc: 72.82%\n",
      "Epoch [146/1000] Train Loss: 0.6979, Train Acc: 74.19% | Test Loss: 0.6931, Test Acc: 75.73%\n",
      "Epoch [147/1000] Train Loss: 0.7029, Train Acc: 74.09% | Test Loss: 0.6843, Test Acc: 73.59%\n",
      "Epoch [148/1000] Train Loss: 0.6849, Train Acc: 74.43% | Test Loss: 0.7714, Test Acc: 72.23%\n",
      "Epoch [149/1000] Train Loss: 0.6632, Train Acc: 75.26% | Test Loss: 0.6919, Test Acc: 74.56%\n",
      "Epoch [150/1000] Train Loss: 0.6762, Train Acc: 74.91% | Test Loss: 0.8049, Test Acc: 71.84%\n",
      "Epoch [151/1000] Train Loss: 0.6991, Train Acc: 74.19% | Test Loss: 0.7406, Test Acc: 72.04%\n",
      "Epoch [152/1000] Train Loss: 0.6750, Train Acc: 74.96% | Test Loss: 0.6496, Test Acc: 76.12%\n",
      "Epoch [153/1000] Train Loss: 0.6542, Train Acc: 75.45% | Test Loss: 0.6491, Test Acc: 76.12%\n",
      "Epoch [154/1000] Train Loss: 0.6629, Train Acc: 75.11% | Test Loss: 0.6553, Test Acc: 77.28%\n",
      "Epoch [155/1000] Train Loss: 0.6721, Train Acc: 74.53% | Test Loss: 0.7390, Test Acc: 73.98%\n",
      "Epoch [156/1000] Train Loss: 0.6969, Train Acc: 74.23% | Test Loss: 0.7100, Test Acc: 72.23%\n",
      "Epoch [157/1000] Train Loss: 0.6692, Train Acc: 74.82% | Test Loss: 0.7146, Test Acc: 72.23%\n",
      "Epoch [158/1000] Train Loss: 0.6691, Train Acc: 74.82% | Test Loss: 0.7227, Test Acc: 73.20%\n",
      "Epoch [159/1000] Train Loss: 0.6806, Train Acc: 74.33% | Test Loss: 0.6711, Test Acc: 74.56%\n",
      "Epoch [160/1000] Train Loss: 0.6671, Train Acc: 74.91% | Test Loss: 0.7689, Test Acc: 72.82%\n",
      "Epoch [161/1000] Train Loss: 0.6998, Train Acc: 73.65% | Test Loss: 0.6644, Test Acc: 76.31%\n",
      "Epoch [162/1000] Train Loss: 0.6493, Train Acc: 75.84% | Test Loss: 0.8268, Test Acc: 68.74%\n",
      "Epoch [163/1000] Train Loss: 0.7290, Train Acc: 72.44% | Test Loss: 0.6663, Test Acc: 75.34%\n",
      "Epoch [164/1000] Train Loss: 0.6685, Train Acc: 75.98% | Test Loss: 0.6484, Test Acc: 75.92%\n",
      "Epoch [165/1000] Train Loss: 0.6487, Train Acc: 75.21% | Test Loss: 0.6907, Test Acc: 73.79%\n",
      "Epoch [166/1000] Train Loss: 0.6882, Train Acc: 74.82% | Test Loss: 0.6501, Test Acc: 74.76%\n",
      "Epoch [167/1000] Train Loss: 0.6291, Train Acc: 76.08% | Test Loss: 0.6831, Test Acc: 72.62%\n",
      "Epoch [168/1000] Train Loss: 0.6415, Train Acc: 75.79% | Test Loss: 0.6730, Test Acc: 75.15%\n",
      "Epoch [169/1000] Train Loss: 0.6544, Train Acc: 76.42% | Test Loss: 0.7142, Test Acc: 75.53%\n",
      "Epoch [170/1000] Train Loss: 0.6382, Train Acc: 75.40% | Test Loss: 0.6710, Test Acc: 75.34%\n",
      "Epoch [171/1000] Train Loss: 0.6662, Train Acc: 75.26% | Test Loss: 0.7016, Test Acc: 74.37%\n",
      "Epoch [172/1000] Train Loss: 0.6277, Train Acc: 76.28% | Test Loss: 0.7231, Test Acc: 72.23%\n",
      "Epoch [173/1000] Train Loss: 0.6643, Train Acc: 75.30% | Test Loss: 0.6515, Test Acc: 74.56%\n",
      "Epoch [174/1000] Train Loss: 0.6469, Train Acc: 75.84% | Test Loss: 0.6314, Test Acc: 76.31%\n",
      "Epoch [175/1000] Train Loss: 0.6358, Train Acc: 76.32% | Test Loss: 0.6345, Test Acc: 75.53%\n",
      "Epoch [176/1000] Train Loss: 0.6282, Train Acc: 77.39% | Test Loss: 0.6558, Test Acc: 74.56%\n",
      "Epoch [177/1000] Train Loss: 0.6424, Train Acc: 76.67% | Test Loss: 0.7242, Test Acc: 76.31%\n",
      "Epoch [178/1000] Train Loss: 0.6797, Train Acc: 74.48% | Test Loss: 0.6333, Test Acc: 76.70%\n",
      "Epoch [179/1000] Train Loss: 0.6386, Train Acc: 77.10% | Test Loss: 0.6948, Test Acc: 75.53%\n",
      "Epoch [180/1000] Train Loss: 0.6384, Train Acc: 76.86% | Test Loss: 0.6440, Test Acc: 76.89%\n",
      "Epoch [181/1000] Train Loss: 0.6278, Train Acc: 76.67% | Test Loss: 0.6474, Test Acc: 74.95%\n",
      "Epoch [182/1000] Train Loss: 0.6294, Train Acc: 76.32% | Test Loss: 0.7244, Test Acc: 73.20%\n",
      "Epoch [183/1000] Train Loss: 0.6090, Train Acc: 77.10% | Test Loss: 0.6377, Test Acc: 76.50%\n",
      "Epoch [184/1000] Train Loss: 0.6377, Train Acc: 76.18% | Test Loss: 0.7177, Test Acc: 72.62%\n",
      "Epoch [185/1000] Train Loss: 0.6516, Train Acc: 75.84% | Test Loss: 0.6429, Test Acc: 76.70%\n",
      "Epoch [186/1000] Train Loss: 0.6622, Train Acc: 75.79% | Test Loss: 0.6598, Test Acc: 75.34%\n",
      "Epoch [187/1000] Train Loss: 0.6156, Train Acc: 76.76% | Test Loss: 0.6678, Test Acc: 74.95%\n",
      "Epoch [188/1000] Train Loss: 0.6106, Train Acc: 77.93% | Test Loss: 0.6132, Test Acc: 76.89%\n",
      "Epoch [189/1000] Train Loss: 0.6545, Train Acc: 75.40% | Test Loss: 0.7159, Test Acc: 74.56%\n",
      "Epoch [190/1000] Train Loss: 0.6308, Train Acc: 76.42% | Test Loss: 0.6364, Test Acc: 75.73%\n",
      "Epoch [191/1000] Train Loss: 0.6371, Train Acc: 76.08% | Test Loss: 0.6392, Test Acc: 76.70%\n",
      "Epoch [192/1000] Train Loss: 0.6053, Train Acc: 77.15% | Test Loss: 0.6378, Test Acc: 76.89%\n",
      "Epoch [193/1000] Train Loss: 0.5941, Train Acc: 78.12% | Test Loss: 0.6544, Test Acc: 74.76%\n",
      "Epoch [194/1000] Train Loss: 0.6069, Train Acc: 77.49% | Test Loss: 0.6440, Test Acc: 76.50%\n",
      "Epoch [195/1000] Train Loss: 0.5905, Train Acc: 78.56% | Test Loss: 0.6133, Test Acc: 77.67%\n",
      "Epoch [196/1000] Train Loss: 0.6100, Train Acc: 77.30% | Test Loss: 0.7029, Test Acc: 73.98%\n",
      "Epoch [197/1000] Train Loss: 0.6166, Train Acc: 77.20% | Test Loss: 0.6955, Test Acc: 74.56%\n",
      "Epoch [198/1000] Train Loss: 0.6223, Train Acc: 76.76% | Test Loss: 0.6767, Test Acc: 75.92%\n",
      "Epoch [199/1000] Train Loss: 0.6082, Train Acc: 77.35% | Test Loss: 0.6339, Test Acc: 78.25%\n",
      "Epoch [200/1000] Train Loss: 0.6061, Train Acc: 77.35% | Test Loss: 0.6306, Test Acc: 77.86%\n",
      "Epoch [201/1000] Train Loss: 0.6131, Train Acc: 76.52% | Test Loss: 0.6727, Test Acc: 77.48%\n",
      "Epoch [202/1000] Train Loss: 0.6100, Train Acc: 77.83% | Test Loss: 0.7190, Test Acc: 74.37%\n",
      "Epoch [203/1000] Train Loss: 0.6166, Train Acc: 77.49% | Test Loss: 0.6453, Test Acc: 75.92%\n",
      "Epoch [204/1000] Train Loss: 0.5989, Train Acc: 78.03% | Test Loss: 0.6261, Test Acc: 76.31%\n",
      "Epoch [205/1000] Train Loss: 0.6069, Train Acc: 77.98% | Test Loss: 0.7812, Test Acc: 73.40%\n",
      "Epoch [206/1000] Train Loss: 0.6171, Train Acc: 77.49% | Test Loss: 0.6356, Test Acc: 75.53%\n",
      "Epoch [207/1000] Train Loss: 0.5894, Train Acc: 78.12% | Test Loss: 0.6310, Test Acc: 77.09%\n",
      "Epoch [208/1000] Train Loss: 0.6097, Train Acc: 77.59% | Test Loss: 0.6432, Test Acc: 77.09%\n",
      "Epoch [209/1000] Train Loss: 0.5969, Train Acc: 78.07% | Test Loss: 0.6359, Test Acc: 76.89%\n",
      "Epoch [210/1000] Train Loss: 0.5743, Train Acc: 78.90% | Test Loss: 0.6399, Test Acc: 77.09%\n",
      "Epoch [211/1000] Train Loss: 0.6045, Train Acc: 77.78% | Test Loss: 0.7170, Test Acc: 73.01%\n",
      "Epoch [212/1000] Train Loss: 0.5650, Train Acc: 78.66% | Test Loss: 0.6261, Test Acc: 77.28%\n",
      "Epoch [213/1000] Train Loss: 0.6088, Train Acc: 78.76% | Test Loss: 0.6774, Test Acc: 76.50%\n",
      "Epoch [214/1000] Train Loss: 0.5677, Train Acc: 78.42% | Test Loss: 0.6255, Test Acc: 75.73%\n",
      "Epoch [215/1000] Train Loss: 0.5611, Train Acc: 79.19% | Test Loss: 0.6239, Test Acc: 76.50%\n",
      "Epoch [216/1000] Train Loss: 0.6008, Train Acc: 77.64% | Test Loss: 0.7126, Test Acc: 72.62%\n",
      "Epoch [217/1000] Train Loss: 0.5637, Train Acc: 79.73% | Test Loss: 0.6101, Test Acc: 77.67%\n",
      "Epoch [218/1000] Train Loss: 0.5800, Train Acc: 77.93% | Test Loss: 0.6214, Test Acc: 78.06%\n",
      "Epoch [219/1000] Train Loss: 0.5748, Train Acc: 78.90% | Test Loss: 0.6998, Test Acc: 75.53%\n",
      "Epoch [220/1000] Train Loss: 0.5847, Train Acc: 78.32% | Test Loss: 0.6634, Test Acc: 76.89%\n",
      "Epoch [221/1000] Train Loss: 0.6055, Train Acc: 77.69% | Test Loss: 0.6918, Test Acc: 74.95%\n",
      "Epoch [222/1000] Train Loss: 0.5927, Train Acc: 78.32% | Test Loss: 0.7426, Test Acc: 73.98%\n",
      "Epoch [223/1000] Train Loss: 0.6007, Train Acc: 77.69% | Test Loss: 0.6675, Test Acc: 76.31%\n",
      "Epoch [224/1000] Train Loss: 0.6040, Train Acc: 77.93% | Test Loss: 0.6260, Test Acc: 76.89%\n",
      "Epoch [225/1000] Train Loss: 0.6118, Train Acc: 77.20% | Test Loss: 0.6388, Test Acc: 77.67%\n",
      "Epoch [226/1000] Train Loss: 0.5988, Train Acc: 78.03% | Test Loss: 0.6926, Test Acc: 76.50%\n",
      "Epoch [227/1000] Train Loss: 0.5606, Train Acc: 79.53% | Test Loss: 0.6370, Test Acc: 75.73%\n",
      "Epoch [228/1000] Train Loss: 0.5463, Train Acc: 79.87% | Test Loss: 0.6132, Test Acc: 77.28%\n",
      "Epoch [229/1000] Train Loss: 0.5764, Train Acc: 79.24% | Test Loss: 0.6568, Test Acc: 75.53%\n",
      "Epoch [230/1000] Train Loss: 0.5670, Train Acc: 78.32% | Test Loss: 0.6153, Test Acc: 76.31%\n",
      "Epoch [231/1000] Train Loss: 0.5774, Train Acc: 79.19% | Test Loss: 0.6482, Test Acc: 76.70%\n",
      "Epoch [232/1000] Train Loss: 0.6455, Train Acc: 76.37% | Test Loss: 0.7168, Test Acc: 74.76%\n",
      "Epoch [233/1000] Train Loss: 0.5398, Train Acc: 79.87% | Test Loss: 0.6352, Test Acc: 76.50%\n",
      "Epoch [234/1000] Train Loss: 0.5354, Train Acc: 80.46% | Test Loss: 0.6647, Test Acc: 75.92%\n",
      "Epoch [235/1000] Train Loss: 0.5698, Train Acc: 78.90% | Test Loss: 0.6598, Test Acc: 77.28%\n",
      "Epoch [236/1000] Train Loss: 0.5490, Train Acc: 79.87% | Test Loss: 0.6470, Test Acc: 75.92%\n",
      "Epoch [237/1000] Train Loss: 0.5454, Train Acc: 79.78% | Test Loss: 0.6578, Test Acc: 73.59%\n",
      "Epoch [238/1000] Train Loss: 0.6157, Train Acc: 77.30% | Test Loss: 0.6267, Test Acc: 77.28%\n",
      "Epoch [239/1000] Train Loss: 0.5338, Train Acc: 80.36% | Test Loss: 0.6587, Test Acc: 75.15%\n",
      "Epoch [240/1000] Train Loss: 0.5880, Train Acc: 77.73% | Test Loss: 0.6313, Test Acc: 77.86%\n",
      "Epoch [241/1000] Train Loss: 0.5180, Train Acc: 80.89% | Test Loss: 0.6259, Test Acc: 75.34%\n",
      "Epoch [242/1000] Train Loss: 0.5516, Train Acc: 79.78% | Test Loss: 0.6246, Test Acc: 76.50%\n",
      "Epoch [243/1000] Train Loss: 0.5290, Train Acc: 80.46% | Test Loss: 0.6277, Test Acc: 77.67%\n",
      "Epoch [244/1000] Train Loss: 0.5091, Train Acc: 81.28% | Test Loss: 0.6100, Test Acc: 79.03%\n",
      "Epoch [245/1000] Train Loss: 0.5363, Train Acc: 80.89% | Test Loss: 0.6251, Test Acc: 76.89%\n",
      "Epoch [246/1000] Train Loss: 0.5582, Train Acc: 79.82% | Test Loss: 0.7887, Test Acc: 72.23%\n",
      "Epoch [247/1000] Train Loss: 0.5663, Train Acc: 78.71% | Test Loss: 0.6069, Test Acc: 78.45%\n",
      "Epoch [248/1000] Train Loss: 0.5103, Train Acc: 80.85% | Test Loss: 0.6190, Test Acc: 77.28%\n",
      "Epoch [249/1000] Train Loss: 0.5435, Train Acc: 79.97% | Test Loss: 0.6373, Test Acc: 76.70%\n",
      "Epoch [250/1000] Train Loss: 0.5923, Train Acc: 78.66% | Test Loss: 0.6719, Test Acc: 75.34%\n",
      "Epoch [251/1000] Train Loss: 0.5352, Train Acc: 80.17% | Test Loss: 0.6340, Test Acc: 76.89%\n",
      "Epoch [252/1000] Train Loss: 0.5843, Train Acc: 78.27% | Test Loss: 0.6308, Test Acc: 76.50%\n",
      "Epoch [253/1000] Train Loss: 0.5585, Train Acc: 78.90% | Test Loss: 0.6321, Test Acc: 77.09%\n",
      "Epoch [254/1000] Train Loss: 0.5375, Train Acc: 79.87% | Test Loss: 0.6940, Test Acc: 74.56%\n",
      "Epoch [255/1000] Train Loss: 0.5498, Train Acc: 79.14% | Test Loss: 0.6734, Test Acc: 73.01%\n",
      "Epoch [256/1000] Train Loss: 0.5177, Train Acc: 81.82% | Test Loss: 0.6675, Test Acc: 75.15%\n",
      "Epoch [257/1000] Train Loss: 0.5074, Train Acc: 81.38% | Test Loss: 0.6055, Test Acc: 78.25%\n",
      "Epoch [258/1000] Train Loss: 0.5111, Train Acc: 80.46% | Test Loss: 0.6237, Test Acc: 76.70%\n",
      "Epoch [259/1000] Train Loss: 0.5304, Train Acc: 80.02% | Test Loss: 0.7964, Test Acc: 72.43%\n",
      "Epoch [260/1000] Train Loss: 0.5521, Train Acc: 79.14% | Test Loss: 0.7227, Test Acc: 72.62%\n",
      "Epoch [261/1000] Train Loss: 0.5387, Train Acc: 79.82% | Test Loss: 0.6432, Test Acc: 76.70%\n",
      "Epoch [262/1000] Train Loss: 0.5122, Train Acc: 80.46% | Test Loss: 0.6298, Test Acc: 77.09%\n",
      "Epoch [263/1000] Train Loss: 0.5079, Train Acc: 80.65% | Test Loss: 0.6419, Test Acc: 76.50%\n",
      "Epoch [264/1000] Train Loss: 0.5095, Train Acc: 80.46% | Test Loss: 0.6444, Test Acc: 75.73%\n",
      "Epoch [265/1000] Train Loss: 0.5328, Train Acc: 79.73% | Test Loss: 0.6648, Test Acc: 77.09%\n",
      "Epoch [266/1000] Train Loss: 0.5537, Train Acc: 80.60% | Test Loss: 0.6658, Test Acc: 75.15%\n",
      "Epoch [267/1000] Train Loss: 0.5317, Train Acc: 79.87% | Test Loss: 0.6322, Test Acc: 77.48%\n",
      "Epoch [268/1000] Train Loss: 0.5024, Train Acc: 81.58% | Test Loss: 0.6511, Test Acc: 76.89%\n",
      "Epoch [269/1000] Train Loss: 0.4873, Train Acc: 81.53% | Test Loss: 0.6921, Test Acc: 74.95%\n",
      "Epoch [270/1000] Train Loss: 0.4975, Train Acc: 81.58% | Test Loss: 0.7180, Test Acc: 74.95%\n",
      "Epoch [271/1000] Train Loss: 0.5433, Train Acc: 79.92% | Test Loss: 0.6174, Test Acc: 76.89%\n",
      "Epoch [272/1000] Train Loss: 0.4993, Train Acc: 81.14% | Test Loss: 0.6433, Test Acc: 75.73%\n",
      "Epoch [273/1000] Train Loss: 0.5027, Train Acc: 81.19% | Test Loss: 0.7159, Test Acc: 76.50%\n",
      "Epoch [274/1000] Train Loss: 0.4935, Train Acc: 81.43% | Test Loss: 0.6447, Test Acc: 77.28%\n",
      "Epoch [275/1000] Train Loss: 0.5435, Train Acc: 78.95% | Test Loss: 0.6232, Test Acc: 76.12%\n",
      "Epoch [276/1000] Train Loss: 0.4923, Train Acc: 81.72% | Test Loss: 0.6593, Test Acc: 76.70%\n",
      "Epoch [277/1000] Train Loss: 0.6062, Train Acc: 76.81% | Test Loss: 0.6352, Test Acc: 75.73%\n",
      "Epoch [278/1000] Train Loss: 0.5119, Train Acc: 80.55% | Test Loss: 0.6199, Test Acc: 78.25%\n",
      "Epoch [279/1000] Train Loss: 0.5128, Train Acc: 80.70% | Test Loss: 0.6736, Test Acc: 75.92%\n",
      "Epoch [280/1000] Train Loss: 0.5124, Train Acc: 80.65% | Test Loss: 0.6721, Test Acc: 76.31%\n",
      "Epoch [281/1000] Train Loss: 0.5045, Train Acc: 82.06% | Test Loss: 0.6435, Test Acc: 76.50%\n",
      "Epoch [282/1000] Train Loss: 0.4877, Train Acc: 81.67% | Test Loss: 0.6269, Test Acc: 76.70%\n",
      "Epoch [283/1000] Train Loss: 0.4624, Train Acc: 83.23% | Test Loss: 0.6182, Test Acc: 78.83%\n",
      "Epoch [284/1000] Train Loss: 0.4994, Train Acc: 80.80% | Test Loss: 0.6318, Test Acc: 77.86%\n",
      "Epoch [285/1000] Train Loss: 0.5237, Train Acc: 80.21% | Test Loss: 0.7644, Test Acc: 69.90%\n",
      "Epoch [286/1000] Train Loss: 0.5919, Train Acc: 77.39% | Test Loss: 0.6908, Test Acc: 75.73%\n",
      "Epoch [287/1000] Train Loss: 0.5430, Train Acc: 80.31% | Test Loss: 0.7008, Test Acc: 76.12%\n",
      "Epoch [288/1000] Train Loss: 0.4893, Train Acc: 81.53% | Test Loss: 0.6435, Test Acc: 77.28%\n",
      "Epoch [289/1000] Train Loss: 0.4629, Train Acc: 82.01% | Test Loss: 0.6250, Test Acc: 77.86%\n",
      "Epoch [290/1000] Train Loss: 0.4674, Train Acc: 82.26% | Test Loss: 0.6388, Test Acc: 77.86%\n",
      "Epoch [291/1000] Train Loss: 0.5027, Train Acc: 81.33% | Test Loss: 0.6301, Test Acc: 77.86%\n",
      "Epoch [292/1000] Train Loss: 0.4652, Train Acc: 82.35% | Test Loss: 0.7550, Test Acc: 72.43%\n",
      "Epoch [293/1000] Train Loss: 0.4809, Train Acc: 81.67% | Test Loss: 0.6760, Test Acc: 77.67%\n",
      "Epoch [294/1000] Train Loss: 0.4677, Train Acc: 82.45% | Test Loss: 0.7300, Test Acc: 73.01%\n",
      "Epoch [295/1000] Train Loss: 0.4758, Train Acc: 82.84% | Test Loss: 0.6416, Test Acc: 76.50%\n",
      "Epoch [296/1000] Train Loss: 0.5213, Train Acc: 80.75% | Test Loss: 0.6411, Test Acc: 76.89%\n",
      "Epoch [297/1000] Train Loss: 0.5177, Train Acc: 80.99% | Test Loss: 0.6688, Test Acc: 77.67%\n",
      "Epoch [298/1000] Train Loss: 0.5165, Train Acc: 81.04% | Test Loss: 0.6428, Test Acc: 77.09%\n",
      "Epoch [299/1000] Train Loss: 0.5242, Train Acc: 80.51% | Test Loss: 0.6852, Test Acc: 76.12%\n",
      "Epoch [300/1000] Train Loss: 0.5731, Train Acc: 79.14% | Test Loss: 0.6849, Test Acc: 76.50%\n",
      "Epoch [301/1000] Train Loss: 0.4635, Train Acc: 82.50% | Test Loss: 0.6481, Test Acc: 78.06%\n",
      "Epoch [302/1000] Train Loss: 0.4714, Train Acc: 82.50% | Test Loss: 0.6872, Test Acc: 76.12%\n",
      "Epoch [303/1000] Train Loss: 0.4538, Train Acc: 82.64% | Test Loss: 0.6677, Test Acc: 77.28%\n",
      "Epoch [304/1000] Train Loss: 0.4720, Train Acc: 82.11% | Test Loss: 0.7803, Test Acc: 75.73%\n",
      "Epoch [305/1000] Train Loss: 0.4891, Train Acc: 82.64% | Test Loss: 0.6535, Test Acc: 78.45%\n",
      "Epoch [306/1000] Train Loss: 0.4655, Train Acc: 83.42% | Test Loss: 0.6976, Test Acc: 75.15%\n",
      "Epoch [307/1000] Train Loss: 0.4513, Train Acc: 83.96% | Test Loss: 0.6185, Test Acc: 77.86%\n",
      "Epoch [308/1000] Train Loss: 0.5912, Train Acc: 78.66% | Test Loss: 0.7289, Test Acc: 75.92%\n",
      "Epoch [309/1000] Train Loss: 0.4750, Train Acc: 82.01% | Test Loss: 0.6464, Test Acc: 79.03%\n",
      "Epoch [310/1000] Train Loss: 0.4475, Train Acc: 83.42% | Test Loss: 0.6447, Test Acc: 79.42%\n",
      "Epoch [311/1000] Train Loss: 0.4203, Train Acc: 84.59% | Test Loss: 0.6563, Test Acc: 77.86%\n",
      "Epoch [312/1000] Train Loss: 0.4423, Train Acc: 83.57% | Test Loss: 0.6563, Test Acc: 78.25%\n",
      "Epoch [313/1000] Train Loss: 0.5006, Train Acc: 81.72% | Test Loss: 0.6682, Test Acc: 77.86%\n",
      "Epoch [314/1000] Train Loss: 0.4406, Train Acc: 84.20% | Test Loss: 0.6702, Test Acc: 76.89%\n",
      "Epoch [315/1000] Train Loss: 0.4667, Train Acc: 82.64% | Test Loss: 0.7364, Test Acc: 73.20%\n",
      "Epoch [316/1000] Train Loss: 0.4801, Train Acc: 81.38% | Test Loss: 0.6521, Test Acc: 78.25%\n",
      "Epoch [317/1000] Train Loss: 0.4340, Train Acc: 83.67% | Test Loss: 0.6713, Test Acc: 77.09%\n",
      "Epoch [318/1000] Train Loss: 0.4740, Train Acc: 82.64% | Test Loss: 0.6832, Test Acc: 76.50%\n",
      "Epoch [319/1000] Train Loss: 0.4478, Train Acc: 83.18% | Test Loss: 0.6486, Test Acc: 78.83%\n",
      "Epoch [320/1000] Train Loss: 0.4500, Train Acc: 82.89% | Test Loss: 0.6707, Test Acc: 78.06%\n",
      "Epoch [321/1000] Train Loss: 0.4429, Train Acc: 83.42% | Test Loss: 0.6718, Test Acc: 78.83%\n",
      "Epoch [322/1000] Train Loss: 0.4303, Train Acc: 83.33% | Test Loss: 0.6827, Test Acc: 77.67%\n",
      "Epoch [323/1000] Train Loss: 0.4095, Train Acc: 84.25% | Test Loss: 0.6822, Test Acc: 76.89%\n",
      "Epoch [324/1000] Train Loss: 0.4222, Train Acc: 83.71% | Test Loss: 0.6731, Test Acc: 76.89%\n",
      "Epoch [325/1000] Train Loss: 0.4735, Train Acc: 82.40% | Test Loss: 0.6770, Test Acc: 77.67%\n",
      "Epoch [326/1000] Train Loss: 0.4114, Train Acc: 83.86% | Test Loss: 0.6795, Test Acc: 75.73%\n",
      "Epoch [327/1000] Train Loss: 0.4525, Train Acc: 83.23% | Test Loss: 0.6505, Test Acc: 78.64%\n",
      "Epoch [328/1000] Train Loss: 0.4745, Train Acc: 82.06% | Test Loss: 0.6845, Test Acc: 77.86%\n",
      "Epoch [329/1000] Train Loss: 0.5373, Train Acc: 79.68% | Test Loss: 0.7177, Test Acc: 75.15%\n",
      "Epoch [330/1000] Train Loss: 0.4544, Train Acc: 82.98% | Test Loss: 0.6418, Test Acc: 78.64%\n",
      "Epoch [331/1000] Train Loss: 0.4347, Train Acc: 84.35% | Test Loss: 0.6341, Test Acc: 77.48%\n",
      "Epoch [332/1000] Train Loss: 0.4192, Train Acc: 84.83% | Test Loss: 0.6756, Test Acc: 75.34%\n",
      "Epoch [333/1000] Train Loss: 0.4109, Train Acc: 84.83% | Test Loss: 0.6277, Test Acc: 78.06%\n",
      "Epoch [334/1000] Train Loss: 0.4337, Train Acc: 83.91% | Test Loss: 0.6713, Test Acc: 77.28%\n",
      "Epoch [335/1000] Train Loss: 0.4083, Train Acc: 85.22% | Test Loss: 0.6824, Test Acc: 76.50%\n",
      "Epoch [336/1000] Train Loss: 0.4284, Train Acc: 84.01% | Test Loss: 0.6824, Test Acc: 77.09%\n",
      "Epoch [337/1000] Train Loss: 0.4042, Train Acc: 85.03% | Test Loss: 0.6896, Test Acc: 76.50%\n",
      "Epoch [338/1000] Train Loss: 0.4791, Train Acc: 81.53% | Test Loss: 0.6715, Test Acc: 78.25%\n",
      "Epoch [339/1000] Train Loss: 0.4470, Train Acc: 83.42% | Test Loss: 0.6550, Test Acc: 77.48%\n",
      "Epoch [340/1000] Train Loss: 0.5194, Train Acc: 80.65% | Test Loss: 0.6738, Test Acc: 77.48%\n",
      "Epoch [341/1000] Train Loss: 0.5029, Train Acc: 80.89% | Test Loss: 0.7187, Test Acc: 73.79%\n",
      "Epoch [342/1000] Train Loss: 0.4425, Train Acc: 82.94% | Test Loss: 0.6818, Test Acc: 78.06%\n",
      "Epoch [343/1000] Train Loss: 0.4352, Train Acc: 83.71% | Test Loss: 0.6680, Test Acc: 76.70%\n",
      "Epoch [344/1000] Train Loss: 0.4315, Train Acc: 84.49% | Test Loss: 0.6623, Test Acc: 78.06%\n",
      "Epoch [345/1000] Train Loss: 0.4004, Train Acc: 85.27% | Test Loss: 0.7231, Test Acc: 76.12%\n",
      "Epoch [346/1000] Train Loss: 0.4410, Train Acc: 83.18% | Test Loss: 0.6819, Test Acc: 78.25%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN_LSTM_2(\n\u001b[1;32m      5\u001b[0m     num_channels\u001b[38;5;241m=\u001b[39mnum_channels, \n\u001b[1;32m      6\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_to_idx),  \u001b[38;5;66;03m# or 'num_classes' if you prefer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     window_length\u001b[38;5;241m=\u001b[39mwindow_length\n\u001b[1;32m      8\u001b[0m )  \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_lstm_model_2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to cnn_lstm_model_2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/EEN210-project/utils/process.py:195\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m    194\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 195\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CNN_LSTM_2(\n",
    "    num_channels=num_channels, \n",
    "    num_classes=len(label_to_idx),  # or 'num_classes' if you prefer\n",
    "    window_length=window_length\n",
    ")  \n",
    "\n",
    "# Train the model\n",
    "model = pp.train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr\n",
    "    )\n",
    "torch.save(model.state_dict(), \"cnn_lstm_model_2.pth\")\n",
    "print(\"Model saved to cnn_lstm_model_2.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
